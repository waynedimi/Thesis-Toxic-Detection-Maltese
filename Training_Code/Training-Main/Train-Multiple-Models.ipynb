{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Family-PC\\miniconda3\\envs\\thesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_and_evaluate_model(model_name, pretrained_model_path, tokenizer_class, model_class, train_data, val_data, batch_size, save_path, max_length=128, accumulation_steps=4, early_stopping_patience=3):\n",
    "    # Load tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    \n",
    "    # Function to tokenize data\n",
    "    def tokenize_data(texts):\n",
    "        return tokenizer([str(text) for text in texts], padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Tokenize the train and validation data\n",
    "    train_encodings = tokenize_data(train_data['comment'].tolist())\n",
    "    val_encodings = tokenize_data(val_data['comment'].tolist())\n",
    "\n",
    "    # Create labels tensors\n",
    "    train_labels = torch.tensor(train_data['isToxic'].values)\n",
    "    val_labels = torch.tensor(val_data['isToxic'].values)\n",
    "\n",
    "    # Create TensorDatasets for train and validation sets\n",
    "    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "    val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Load model\n",
    "    model = model_class.from_pretrained(pretrained_model_path, num_labels=1)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize optimizer, loss, and gradient scaler\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 50\n",
    "    total_batches = len(train_dataloader)\n",
    "    print_every = max(1, total_batches // 10)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Scheduler: Linear warmup and decay\n",
    "    num_training_steps = num_epochs * total_batches // accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Create directory to save the best model\n",
    "    model_save_dir = '../../temp/'\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_save_dir, 'best_model.pth')\n",
    "    custom_save_dir = save_path\n",
    "    os.makedirs(custom_save_dir, exist_ok=True)\n",
    "\n",
    "    metrics_filename = os.path.join(custom_save_dir, 'metrics.csv')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    with open(metrics_filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Epoch', 'Train Loss', 'Val Loss', 'Train Accuracy', 'Val Accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                with record_function(\"train_epoch\"):\n",
    "                    for batch_idx, (input_ids, attention_mask, target) in enumerate(train_dataloader):\n",
    "                        input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "                        target = target.unsqueeze(1).float()\n",
    "\n",
    "                        with autocast():\n",
    "                            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                            loss = criterion(outputs.logits, target)\n",
    "                            loss = loss / accumulation_steps\n",
    "                            scaler.scale(loss).backward()\n",
    "\n",
    "                        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == total_batches:\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                        preds = torch.sigmoid(outputs.logits).detach().cpu().numpy()\n",
    "                        preds = (preds > 0.5).astype(int)\n",
    "                        all_predictions.extend(preds)\n",
    "                        all_targets.extend(target.detach().cpu().numpy())\n",
    "\n",
    "                        if batch_idx % print_every == 0:\n",
    "                            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{total_batches}: Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "            epoch_loss = running_loss / total_batches\n",
    "            epoch_accuracy = accuracy_score(np.vstack(all_predictions), np.vstack(all_targets))\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for input_ids, attention_mask, target in val_dataloader:\n",
    "                    input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "                    target = target.unsqueeze(1).float()\n",
    "\n",
    "                    with autocast():\n",
    "                        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                        loss = criterion(outputs.logits, target)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    preds = torch.sigmoid(outputs.logits).detach().cpu().numpy()\n",
    "                    preds = (preds > 0.5).astype(int)\n",
    "                    val_predictions.extend(preds)\n",
    "                    val_targets.extend(target.detach().cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = accuracy_score(np.vstack(val_predictions), np.vstack(val_targets))\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Save metrics to CSV\n",
    "            writer.writerow({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Train Loss': epoch_loss,\n",
    "                'Val Loss': val_loss,\n",
    "                'Train Accuracy': epoch_accuracy,\n",
    "                'Val Accuracy': val_accuracy\n",
    "            })\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    prof.export_stacks(\"profiler_stacks.txt\", \"cpu\")\n",
    "    prof.export_stacks(\"profiler_stacks_gpu.txt\", \"cuda\")\n",
    "\n",
    "    print(f\"Best epoch: {best_epoch + 1}, Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    # Load the best model and save it\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.save_pretrained(custom_save_dir)\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_epoch': best_epoch + 1,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "def check_overfitting(metrics):\n",
    "    best_epoch = metrics['best_epoch'] - 1\n",
    "    if metrics['train_accuracies'][best_epoch] > metrics['val_accuracies'][best_epoch] + 0.05:\n",
    "        print(f\"Model {metrics['model_name']} might be overfitting. Training accuracy is significantly higher than validation accuracy at the best epoch.\")\n",
    "    else:\n",
    "        print(f\"Model {metrics['model_name']} does not show significant signs of overfitting.\")\n",
    "\n",
    "def plot_metrics(models_metrics):\n",
    "    # Plot validation accuracy per epoch for all models\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for metrics in models_metrics:\n",
    "        plt.plot(metrics['val_accuracies'], label=f\"{metrics['model_name']} - Validation Accuracy\")\n",
    "    plt.title('Validation Accuracy per Epoch for All Models', fontsize=20)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=16)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss per epoch for each model separately\n",
    "    for metrics in models_metrics:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(metrics['train_losses'], label='Training Loss', color='blue', linewidth=2, marker='o', markersize=5)\n",
    "        plt.plot(metrics['val_losses'], label='Validation Loss', color='orange', linewidth=2, marker='s', markersize=5)\n",
    "        best_epoch_idx = metrics['best_epoch']\n",
    "        plt.axvline(best_epoch_idx, linestyle='--', color='green', label='Best Epoch', linewidth=1.5)\n",
    "        plt.annotate(f'Best Epoch: {best_epoch_idx}', xy=(best_epoch_idx, metrics['best_val_loss']), xytext=(best_epoch_idx + 2, metrics['best_val_loss'] + 0.02),\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12, color='green')\n",
    "        plt.title(f'{metrics[\"model_name\"]} - Loss per Epoch', fontsize=20)\n",
    "        plt.xlabel('Epoch', fontsize=16)\n",
    "        plt.ylabel('Loss', fontsize=16)\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('../../Datasets/Dataset_English/train/train.csv')\n",
    "val_data = pd.read_csv('../../Datasets/Dataset_English/val/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for null values...\n",
      "comment      102\n",
      "comment.1    102\n",
      "isToxic        0\n",
      "dtype: int64\n",
      "                                             comment  \\\n",
      "0  outrageous block outrageous un wiki lauren cai...   \n",
      "1  except never dare say something new neil harbo...   \n",
      "2  thanks reply explanation clarified issue perfe...   \n",
      "3                           attempted generalization   \n",
      "4     seem vandalising moving reference stupid place   \n",
      "\n",
      "                                           comment.1  isToxic  \n",
      "0  outrageous block outrageous un wiki lauren cai...        0  \n",
      "1  except never dare say something new neil harbo...        0  \n",
      "2  thanks reply explanation clarified issue perfe...        0  \n",
      "3                           attempted generalization        0  \n",
      "4     seem vandalising moving reference stupid place        1  \n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Checking for null values...\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Fill or drop null values (choose one based on your preference)\n",
    "train_data['comment'].fillna('', inplace=True)  # Fill with empty string\n",
    "# train_data.dropna(subset=['comment'], inplace=True)  # Or drop rows with null comments\n",
    "\n",
    "# Ensure all comments are strings\n",
    "train_data['comment'] = train_data['comment'].astype(str)\n",
    "\n",
    "# Verify the data again\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Family-PC\\miniconda3\\envs\\thesis\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MLRS/mBERTu and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/8976: Loss: 0.7101\n"
     ]
    }
   ],
   "source": [
    "models_metrics = []\n",
    "models_to_train = [\"MLRS/mBERTu\", \"bert-base-uncased\", \"xlm-roberta-base\", \"roberta-base\"]\n",
    "custom_paths = [\n",
    "    \"../../models/Experiments/Validation/Experiment-3/mBERTu_ENG\",\n",
    "    \"../../models/Experiments/Validation/Experiment-3/BERT_ENG\",\n",
    "    \"../../models/Experiments/Validation/Experiment-3/XLM-R_ENG\",\n",
    "    \"../../models/Experiments/Validation/Experiment-3/RoBERTa_ENG\"\n",
    "]\n",
    "batch_sizes = [16 , 16 , 16 , 16]\n",
    "\n",
    "for model_name, custom_path , batch in zip(models_to_train, custom_paths , batch_sizes):\n",
    "    metrics = train_and_evaluate_model(\n",
    "        model_name=model_name,\n",
    "        pretrained_model_path=model_name,\n",
    "        tokenizer_class=AutoTokenizer,\n",
    "        model_class=AutoModelForSequenceClassification,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        batch_size=batch,\n",
    "        save_path=custom_path,\n",
    "        max_length=128,\n",
    "        accumulation_steps=4,\n",
    "        early_stopping_patience=4\n",
    "    )\n",
    "    models_metrics.append(metrics)\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(models_metrics)\n",
    "\n",
    "# Check for overfitting\n",
    "for metrics in models_metrics:\n",
    "    check_overfitting(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
